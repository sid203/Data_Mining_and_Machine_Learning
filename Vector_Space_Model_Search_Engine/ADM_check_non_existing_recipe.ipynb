{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "#Crawl for Cuisines\n",
    "home_link='http://www.bbc.co.uk'\n",
    "start_link=\"http://www.bbc.co.uk/food/recipes\"\n",
    "#Extracting the data from the webpages\n",
    "\n",
    "def extract_method(rsoup):\n",
    "    z_tmp=rsoup.find_all('p',{'class':'recipe-method__list-item-text'})\n",
    "    txt=[]\n",
    "    for i in z_tmp:\n",
    "        txt.append(i)\n",
    "        \n",
    "    method_info=''\n",
    "    for tag in txt:\n",
    "        b=BeautifulSoup(str(tag))\n",
    "        method_info=method_info+(b.find('p').contents[0])\n",
    "        \n",
    "    return ' '.join(method_info.split())\n",
    "\n",
    "\n",
    "def extract_author(rsoup):\n",
    "    result = ''\n",
    "    for tag in rsoup.find_all(itemprop='author'):\n",
    "        result = tag.contents[0]\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_cooking_time(rsoup):\n",
    "    result = ''\n",
    "    for tag in rsoup.find_all(itemprop='cookTime'):\n",
    "        result = tag.contents[0]\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_dietary_information(rsoup):\n",
    "    s=\"Vegetarian\"\n",
    "    items = rsoup.find_all(\"div\", {\"class\":\"recipe-metadata__dietary\"})\n",
    "    if bool(items)==True:\n",
    "        for i in items:\n",
    "            x = i.find(\"p\", {\"class\": \"recipe-metadata__dietary-vegetarian-text\"}).get_text()\n",
    "            if s in x:\n",
    "                return(s)      \n",
    "        \n",
    "    else:\n",
    "        return(\"No information\")\n",
    "    \n",
    "def extract_ingredients(rsoup):\n",
    "    \n",
    "    items = rsoup.find_all(\"ul\", {\"class\":\"recipe-ingredients__list\"})\n",
    "    ingredients_info=''\n",
    "    if bool(items)==True:\n",
    "        for i in items:\n",
    "            x = i.find_all(\"li\", {\"class\": \"recipe-ingredients__list-item\"})\n",
    "            for j in x:\n",
    "                ingredients_info+=(j.get_text())\n",
    "                \n",
    "    else:\n",
    "        return ('No Information')\n",
    "\n",
    "    return ' '.join(ingredients_info.split())\n",
    "\n",
    "    \n",
    "def extract_prep_time(rsoup):\n",
    "    result = ''\n",
    "    for tag in rsoup.find_all(itemprop='prepTime'):\n",
    "        result = tag.contents[0]\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_program_information(rsoup):\n",
    "    \n",
    "    items = rsoup.find_all(\"div\", {\"class\":\"recipe-is-from-widget__about\"})\n",
    "    if bool(items)==True:\n",
    "        for i in items:\n",
    "            x = i.find(\"p\", {\"class\": \"recipe-is-from-widget__programme-series-title\"}).get_text()\n",
    "            return(x.strip())\n",
    "        \n",
    "    else:\n",
    "        return(\"No information\")\n",
    "\n",
    "def extract_serves(rsoup):\n",
    "    result = ''\n",
    "    for tag in rsoup.find_all(itemprop='recipeYield'):\n",
    "        result = tag.contents[0]\n",
    "    return result\n",
    "\n",
    "def extract_title(rsoup):\n",
    "    result = ''\n",
    "    for tag in rsoup.find_all(itemprop='name'):\n",
    "        result = tag.contents[0]\n",
    "    return result\n",
    "\n",
    "#File management functions\n",
    "\n",
    "def collect_all_information(url):\n",
    "    ## Use the Functions to get the Data #\n",
    "    try:\n",
    "        recipe=requests.get(url)\n",
    "    except requests.exceptions.RequestException as e:  # This is the correct syntax\n",
    "        print(e)\n",
    "        sys.exit(1)\n",
    "    rsoup = BeautifulSoup(recipe.text, \"lxml\")\n",
    "\n",
    "    Single_Recipe_Data=[]\n",
    "    auth=extract_author(rsoup)\n",
    "    p_time=extract_prep_time(rsoup)\n",
    "    c_time=extract_cooking_time(rsoup)\n",
    "    sv=extract_serves(rsoup)\n",
    "    dietary=extract_dietary_information(rsoup)\n",
    "    progrm=extract_program_information(rsoup)\n",
    "    ing=extract_ingredients(rsoup)\n",
    "    m=extract_method(rsoup)\n",
    "    t=extract_title(rsoup)\n",
    "    # Save the Data together in one, Variable , and return that Variable\n",
    "    Single_Recipe_Data.append(t)\n",
    "    Single_Recipe_Data.append(auth)\n",
    "    Single_Recipe_Data.append(p_time)\n",
    "    Single_Recipe_Data.append(c_time)\n",
    "    Single_Recipe_Data.append(dietary)\n",
    "    Single_Recipe_Data.append(progrm)\n",
    "    Single_Recipe_Data.append(sv)\n",
    "    Single_Recipe_Data.append(ing)\n",
    "    Single_Recipe_Data.append(m)\n",
    "    \n",
    "    return Single_Recipe_Data\n",
    "\n",
    "def write_to_file(Recipe,file_name):\n",
    "    target=open(file_name,'a',encoding='utf-8')\n",
    "    for element in Recipe:\n",
    "        target.write(str(element))\n",
    "        target.write('\\n')\n",
    "    target.close()\n",
    "    \n",
    "\n",
    "def read_all_links(file_name):\n",
    "\n",
    "    target=open(file_name, 'r',encoding='utf-8')\n",
    "    links=[wrd.strip() for wrd in target]\n",
    "\n",
    "    target.close()\n",
    "    return links\n",
    "\n",
    "\n",
    "def extract_all_information(batch,c):\n",
    "    #d=make_batches('all_recipe_links.txt')\n",
    "    path = './ADM_dataset/test/___'\n",
    "    #c=0\n",
    "    #dt=0\n",
    "    #for batch in d:\n",
    "    #    print('Batch num'+str(dt)+'starting.....')\n",
    "    for link in batch:\n",
    "        print(link)\n",
    "        print('\\n')\n",
    "        print(c)\n",
    "        single_recipe=collect_all_information(link)\n",
    "        write_to_file(single_recipe,path+str(c)+'.txt')\n",
    "        time.sleep(1)\n",
    "        c=c+1\n",
    "        #print('Batch num '+str(dt)+'complete !!')\n",
    "        #dt=dt+1\n",
    "    return 'done'\n",
    "\n",
    "def make_batches(file_name_links,n=1000):\n",
    "    l = read_all_links(file_name_links)\n",
    "    k=[l[i:i + n] for i in range(0, len(l), n)]\n",
    "    return k\n",
    "\n",
    "d=make_batches('all_recipe_links.txt')\n",
    "all_links = read_all_links('all_recipe_links.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = './ADM_dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "lst=[]\n",
    "for i in range(11269):\n",
    "    statinfo = os.stat(path+'/___'+str(i)+'.txt')\n",
    "    if statinfo.st_size < 100:\n",
    "        lst.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
