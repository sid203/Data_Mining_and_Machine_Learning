{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import string\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import ADM_heapfunctions as heap\n",
    "\n",
    "#Clean the given query (does not filter out duplicate words but it is taken care of in the program)\n",
    "def clean_query(lst):\n",
    "    stop_words =stopwords.words('english') + list(punctuation)+list(['no','information','NO','No','Information','INFORMATION'])\n",
    "    words_lst = [nltk.word_tokenize(sentence) for sentence in lst]\n",
    "    words=[]\n",
    "    for wrd in words_lst:\n",
    "        words=words + wrd\n",
    "    \n",
    "    words = [w.lower() for w in words]\n",
    "    words = [PorterStemmer().stem_word(word) for word in words]\n",
    "    return [w for w in words if w not in stop_words and not w.isdigit()]\n",
    "\n",
    "\n",
    "#Read all docs from the disk into a dictionary \n",
    "def docs_dct():\n",
    "    num_docs=11269\n",
    "    all_docs_dct={}\n",
    "    for i in range(num_docs):\n",
    "        all_docs_dct[i]=read_doc(i)\n",
    "\n",
    "    return all_docs_dct\n",
    "\n",
    "\n",
    "#Sub function of docs_dct(), read one single document from the disk\n",
    "def read_doc(i):\n",
    "    path_file = './ADM_dataset/cleaned/'+'___'\n",
    "    doc=[]\n",
    "    target=open(path_file+str(i)+'.txt','r',encoding='utf-8')\n",
    "    for wrd in target:\n",
    "        doc.append(wrd.rstrip('\\n'))\n",
    "    return doc\n",
    "\n",
    "\n",
    "#TF-IDF Functions\n",
    "\n",
    "#Calculates term frequency in a document. The type of variable document must be string for this function to work\n",
    "def tf(term, document):\n",
    "    return 1+(np.log(freq(term, document))/np.log(10)) #1+log(tf) logarithmic term frequency\n",
    "\n",
    "#sub function of tf() function\n",
    "def freq(term, document):\n",
    "    return document.split().count(term)\n",
    "\n",
    "#calculates the inverse doc freq of a term log(N/df_n)\n",
    "def idf(term):\n",
    "    num_docs=11269\n",
    "    idf_weight=np.log(num_docs/len(inverted_index[term]))/np.log(10)\n",
    "    return idf_weight\n",
    "\n",
    "\n",
    "#converts the given document having list of words into a string (so that it can be used by tf() function)\n",
    "#and also builds unique words out of the given document list\n",
    "def build_uni_wrds_cnv_string(doc_id_list):\n",
    "    lexicon_doc=set()\n",
    "    for word in doc_id_list:\n",
    "        lexicon_doc.update([word])\n",
    "    \n",
    "    ch=''\n",
    "    for j in doc_id_list:\n",
    "        ch=ch+' '+j\n",
    "    #returns unique words in lexicon_doc and converts list to string, to be used by tf( function)        \n",
    "    return lexicon_doc,ch  \n",
    "\n",
    "#converts an input query into a vector space\n",
    "def query_vector(query):\n",
    "    uniq_wrds,ch=build_uni_wrds_cnv_string(query)\n",
    "    query_vec={}\n",
    "    query_vec_list=[]\n",
    "    for term in uniq_wrds:\n",
    "        try:\n",
    "            query_vec[term]=tf(term,ch)*idf(term)\n",
    "            query_vec_list.append(query_vec[term])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    mod_val=np.sqrt(np.sum(np.array(query_vec_list)**2)) #to normalize the query vector (there is no need for this,remove it)\n",
    "    \n",
    "    for key in query_vec.keys():\n",
    "        query_vec[key]=query_vec[key]/mod_val #normalized query vector\n",
    "        \n",
    "    return query_vec\n",
    "\n",
    "\n",
    "#converts input document list into vector space\n",
    "def doc_vector(doc_id_list):\n",
    "    uniq_words,ch=build_uni_wrds_cnv_string(doc_id_list)\n",
    "    tf_vec={}\n",
    "    tf_vec_list=[]\n",
    "    for word in uniq_words:\n",
    "        tf_vec[word]=tf(word,ch) # no need to multiply by idf weight here as it is already done in query vector\n",
    "        tf_vec_list.append(tf_vec[word])\n",
    "        \n",
    "    mod_tf = np.sqrt(np.sum(np.array(tf_vec_list)**2))\n",
    "    \n",
    "    for key in tf_vec.keys():\n",
    "        tf_vec[key]=tf_vec[key]/mod_tf\n",
    "        \n",
    "    return tf_vec\n",
    "        \n",
    "    \n",
    "#computes the cosine score between query_vector and document_vector\n",
    "def compute_cosine(query_vec_dct,doc_vec_dct):\n",
    "    uniq_words_query=set(query_vec_dct.keys())\n",
    "    uniq_words_doc=set(doc_vec_dct.keys())\n",
    "    score=0\n",
    "    for val in (uniq_words_doc & uniq_words_query):\n",
    "        score+=query_vec_dct[val]*doc_vec_dct[val]\n",
    "    return score\n",
    "\n",
    "\n",
    "#finds posting list for the terms of given query\n",
    "def find_posting_list_query(query_list):\n",
    "    dct={}\n",
    "    for term in query_list:\n",
    "        try:\n",
    "            dct[term]=inverted_index[term]\n",
    "        except:\n",
    "            pass\n",
    "    return dct\n",
    "\n",
    "#computes cosine score for all documents in the postings list, however they are unsorted at this point\n",
    "def rank_docs(query):\n",
    "    doc_dct=find_posting_list_query(query)\n",
    "    score_dct={}\n",
    "    doc_ids=[]\n",
    "    for key in doc_dct.keys():\n",
    "        doc_ids+=[a[0] for a in doc_dct[key]]\n",
    "\n",
    "    #precompute query vector as it will be same for all calculations\n",
    "    query_vec=query_vector(query)\n",
    "    \n",
    "    for document in doc_ids:\n",
    "        score_dct[document]=compute_cosine(query_vec,doc_vector(document_dct[document]))\n",
    "    \n",
    "    return score_dct\n",
    "\n",
    "#outputs the list of documents and asks to enter the query\n",
    "def run_query():\n",
    "    input_query=str(input())\n",
    "    query=clean_query(input_query.split())\n",
    "    dct=rank_docs(query)\n",
    "    inv_dct = {v: k for k, v in dct.items()}\n",
    "    score_array=np.array(list(inv_dct.keys()))\n",
    "    sorted_scores=heap.heapsort(score_array)\n",
    "    #print(sorted_scores.shape)\n",
    "    links=get_links()\n",
    "    for i in range(len(sorted_scores)-1,len(sorted_scores)-20,-1):\n",
    "        #print(i)\n",
    "        print('Score:',sorted_scores[i],'Doc_ID',inv_dct[sorted_scores[i]])\n",
    "        print('Link',links[inv_dct[sorted_scores[i]]])\n",
    "\n",
    "#collects all links in memory\n",
    "def get_links():\n",
    "    path_file = 'all_recipe_links.txt'\n",
    "    doc=[]\n",
    "    target=open(path_file,'r',encoding='utf-8')\n",
    "    for wrd in target:\n",
    "        doc.append(wrd.rstrip('\\n'))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverted_index=pickle.load(open('inverted_index.p','rb'))\n",
    "document_dct=docs_dct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6988"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inverted_index['oil'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oil\n",
      "Score: 0.244401015741 Doc_ID 8044\n",
      "Link http://www.bbc.co.uk/food/recipes/goatscheesepacirctea_65619\n",
      "Score: 0.241371061921 Doc_ID 1524\n",
      "Link http://www.bbc.co.uk/food/recipes/halloumicheeselemono_13052\n",
      "Score: 0.237636903292 Doc_ID 3003\n",
      "Link http://www.bbc.co.uk/food/recipes/sun-blushed_tomato_and_23591\n",
      "Score: 0.236249332952 Doc_ID 5843\n",
      "Link http://www.bbc.co.uk/food/recipes/homemade_chilli_oil_54985\n",
      "Score: 0.231916140991 Doc_ID 4880\n",
      "Link http://www.bbc.co.uk/food/recipes/steamedbroccoliwithl_84194\n",
      "Score: 0.231772888613 Doc_ID 10890\n",
      "Link http://www.bbc.co.uk/food/recipes/breadedcheese_77127\n",
      "Score: 0.228559628961 Doc_ID 9680\n",
      "Link http://www.bbc.co.uk/food/recipes/steamedseabasswithgi_90422\n",
      "Score: 0.22707566237 Doc_ID 6415\n",
      "Link http://www.bbc.co.uk/food/recipes/tomato_rubbed_toast_65992\n",
      "Score: 0.225390709742 Doc_ID 3755\n",
      "Link http://www.bbc.co.uk/food/recipes/tomatosoup_75886\n",
      "Score: 0.222202789967 Doc_ID 9158\n",
      "Link http://www.bbc.co.uk/food/recipes/lambcutletswithgarli_83750\n",
      "Score: 0.213996265502 Doc_ID 4756\n",
      "Link http://www.bbc.co.uk/food/recipes/gamechips_79105\n",
      "Score: 0.213970806226 Doc_ID 9910\n",
      "Link http://www.bbc.co.uk/food/recipes/sesame_seed_salmon_with_59761\n",
      "Score: 0.213763427554 Doc_ID 2507\n",
      "Link http://www.bbc.co.uk/food/recipes/parsleypestowithpota_11633\n",
      "Score: 0.211330343287 Doc_ID 8130\n",
      "Link http://www.bbc.co.uk/food/recipes/saladofpigeonandcara_78465\n",
      "Score: 0.210813486389 Doc_ID 118\n",
      "Link http://www.bbc.co.uk/food/recipes/griddledsmokedsalmon_78254\n",
      "Score: 0.209683904663 Doc_ID 3857\n",
      "Link http://www.bbc.co.uk/food/recipes/sesamecrustedscallop_86818\n",
      "Score: 0.209615386204 Doc_ID 10324\n",
      "Link http://www.bbc.co.uk/food/recipes/bruschettawithhallou_85353\n",
      "Score: 0.209262701986 Doc_ID 8100\n",
      "Link http://www.bbc.co.uk/food/recipes/almonddippingsaucewi_88469\n",
      "Score: 0.208804790447 Doc_ID 628\n",
      "Link http://www.bbc.co.uk/food/recipes/pepperedgriddledgoat_78709\n"
     ]
    }
   ],
   "source": [
    "run_query()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
